# 1. CASE STUDIES AND TALES OF WOE
----------------------------------
* Punishing competence with responsibility
------------------------------------------
Congratulations, you've earned a promotion or a new job. You've impressed those around you with your experience and ability to master your engineering skills. 

As you're going through your new responsibilities, something stands out, an old critical system that nobody knows very well, if at all. Good news, the thing nobody else wanted to deal 
with is now your problem. 

Somebody has to own it and it might as well be you. 

The intent may be a test of your skills. You've done well so far, but you're not quite at the next level. This one last task will show whether you've got the temperament, spirit, and 
skills for that next promotion. 

An opportunity to show off what you've learned and gain some additional experience along the way. 

Alternatively, this dubious gift could be seen as a compliment. You're so good that you can handle any challenge and nobody else is capable of conquering the beast. 

This is an opportunity to shine or be knocked down a peg or two. Does it truly matter how you got to this point? Not really, to be honest. What matters more is your strategy from this 
point forward. 

You have unique opportunity to maintain and establish a new chapter in the book of this project. There could be many more or this could be the end of this poor, decrepit program. 

What exactly is legacy software? In computing, legacy software is software superseded by another system but is still in widespread use and therefore difficult to replace. 

Here are some practical examples of legacy software applications. 

A content management system built with the previous generation of both the language and framework, yet is used daily by hundreds who relied on it for their livelihood. 

An affiliate system with tens of thousands of users that could not ingest data quickly enough to calculate. Each passing day introduced a greater data backlog and more frustrated 
affiliates demanding accurate data and payouts. 

A monolithic website publishing system that included both an editor and the front end that hadn't been deployed to production in years, but couldn't be turned off out of fear from the 
last outage that was caused when it was attempted. 

An open source project with thousands of users, an issue queue with scores of issues, bug reports and patches that was abandoned by its primary author and maintainer. 

Each of these systems was important at one point and still is. However, they are no longer the latest and greatest. 

Teams or individual engineers consider them burdensome to maintain. 

Each was important to its audience. The opportunity and responsibility of owning and managing a legacy system is a pivotal moment. Your approach from the very beginning will set the tone 
for the duration of your experience.
-------------------------------------------------------------------------------------------------------------------------------------------------------------------

* Respecting history
--------------------
Software is written and developed by people, typically with best intentions. 

They may be professional, they may be amateur, and they may be writing a program for the very first time. 

Regardless of their journey, they have created something tangible that can be used. People have feelings, hopes, and dreams and pride in their accomplishments. 

Engineering can be both difficult and rewarding. The end result may be a utility to solve a specific problem. It may be a shared solution within a department. It could even be a 
commercial product with paying customers. 

Regardless of what type of system it is, it was built with a purpose. The requirements and intents that led to the creation of the service may be lost with time. 

They may be perfectly well documented. There's a very good chance that a system is being used in a different way than what was originally intended. 

If a system seems unmanageable now, take a deep breath. 

Try to empathize with those who came before you. When writing software, nobody creates a dumpster fire on purpose. 

You could say that the road to sorrow is paved with good intentions, and acting in good faith doesn't guarantee success. 

Something that I've learned in an excruciating and visceral way is that you should assume the best intentions of the people and the team that built a piece of software. 

Badmouthing a piece of software can be synonymous with insulting the people who wrote it. There's an excellent chance that someone involved with that project is still around to hear your 
criticism of it. 

Would you walk up to someone and tell them that they're terrible engineers? No? Then why would you say that their work was awful? The result of their blood, sweat, and tears has no value. 

If a system works, even if it's subjectively ugly and weird it still works and should be respected. If it didn't work, it'd be turned off or retired, or one would assume. 

More on that later. The best practices of two, five, or 10 years ago may be different from those of today. The dependencies of the system may be outdated. 

It may rely on an outdated version of a database with dozens of critical security vulnerabilities, yet it still performs its duty. 

It may be challenging to maintain or to add on, or more accurately, those who are responsible for it don't know how to maintain it. 

Somebody built it and got it to the point where it is today. It's less important to be proper or academically correct. 

Subjectively and objectively, you may be accurate but the system may be imperfect. Does it matter? Not really. Could it be done better? Sure. 

Is that a point worth making? Absolutely not. Assume the best intentions, respect what came before, and look forward to the next chapter in the application's legacy. 

As you assume ownership of a legacy system, documentation will play a key role in your understanding. So what type of manual should you be looking for and writing?

------------------------------------------------------------------------------------------------------------------------------------------------------------------

* Effective documentation
-------------------------
Effective documentation. Writing documentation is a task that's easy to procrastinate. It's fun to build and plan things, but there's little glory in writing documentation. 

A good manual is an essential component of a healthy system. What does effective documentation look like? A good manual should be both thorough and concise, which sounds contradictory. 

There's a case for both. The thoroughness should answer essential questions about the system. 

What does the application do? 

Who maintains the software and is responsible for it? 

Why was it created? 

How does it work? 

How do you know it's working correctly? 

What are the troubleshooting steps if it isn't working properly? 

Make sure that there's enough information to answer the most important questions. This is a delicate balance. 

When is there too much information? 

The conciseness should be the brevity of information. Get to the point and get out of the way. 

Consider a cooking blog that stretches a recipe into pages of descriptions of how dill makes you feel and helpful affiliate links to every kitchen gadget known to humanity. 

If you just want to make dinner and see a recipe without having to scroll, all that commentary and distractions are exactly just that. 

An effective manual makes its point and gets out of the way. 

It doesn't have to be lengthy, just informative. So how do you find a balance between too much and too little documentation? 

Logical organization and priority hierarchy will improve navigation. 

Brief statements with logical paragraph breaks, concise lists, and clean formatting really make understanding the system easier for others. 

The goal should be to educate. 

If there was an emergency and you weren't available, 

would someone else know what to do if they read the manual? 

Would they know how to find it? 

Is it spread out in multiple locations and should be consolidated to save effort? 

If a piece of software is part of a larger system, it's definitely recommended to link to the larger systems documentation and avoid duplication. 

For example, deployment instructions may be the same for a collection of applications. So just refer to the main instructions and only document exceptions or specific configurations. 

Throughout your journey to own a system, you should be recording your findings in a manual, knowledge base, wiki, readme, or some other centralized and accessible location. 

Save the next person the time. There's no advantage to siloing information. And memory is fallible. The longer you wait to write it down, the more likely that details will be missed. 

Have you heard the lazy excuse that a program is so perfectly designed that it's self-documenting? If you've made that excuse, I hope you feel a twinge of shame. 

Code should be commented and logical, but where's the description of the overall intent, how to manage the system? Something is obviously better than nothing, but a well-documented 
interior and an obtuse confusing exterior are a detriment to anyone trying to maintain the system. 

Build the knowledge base to save the next person time and prevent information siloing. As you learn more about a system, organize and document it. 

An important question that should be documented is, who is the actual owner of the system?


------------------------------------------------------------------------------------------------------------------------------------------------------------------

* Who actually needs this?
--------------------------
Who actually needs this? People come and go from organizations regularly. It's a natural part of professional life. 

Like cells in your body that are constantly rejuvenating and replacing themselves, over time an organization with the same name and departments can have completely different faces. 

That's not necessarily a bad thing. It just is. However, it also means that with different people come different needs. 

Just because a particular process or application was important five years ago doesn't mean that it's still relevant today. 

Tooling may have improved, standards and technologies change, and audiences can be fickle. 

It's natural for systems to come and go as needs shift over time. Change for the sake of change isn't always good but neither is refusing to adjust despite what the market is telling you. 

Based on the research and conversations you've been having, you should have a comprehensive understanding of the history, intended audience, scope, and stakeholders. 

You can compare that to the actual usage of the application and its current audience. 

Validate their understanding of what you've learned and what is current. 

If it's aligned, great. If it's not aligned, there might be some bigger and more difficult conversations that need to be had. 

Here's an example of this sort of situation. There is a content ingester that would read articles from a feed, translate them into a different language, and then add them, unpublished, 
into the content management system. 

Our team had written and owned it, so we knew its history. Over time, the volume of content translated would increase to the point that we started paying overages to the translation 
service. 

We checked with stakeholders and decision-makers and they insisted that, yes, this was an important part of our strategy and we still needed to maintain it. 

So we continue to patch and keep it up to date. As part of a different project, we decided to re-implement the system and started reverse-engineering it. 

We took a look at the content that had been added and discovered that nothing had been published for months. 

What was the root cause? The people who were doing the publishing changed their strategy but never notified our liaison about the change. 

The system was technically doing exactly what it had been designed to do: read, translate, and send content over. 

Nobody complained, so we just kept it going and paid for the translation. 

The reflexive trust cost the company thousands of dollars before the situation was detected and the service was gracefully shut down, leaving a large volume of unpublished content that 
adds to the bulk of the database and serves no other practical purpose. 

This was avoidable with good communication across departments and change management. 

Effective monitoring, not just the artifact created by the system but the intended result, could have surfaced the issue earlier. 

This was an embarrassing and frustrating incident, representing a real cost, both in money and time, that could've been used elsewhere with greater impact. 

There were good intentions all around and some opportunities for improvement, in hindsight. The moral of the story? When it comes to legacy applications, trust the information you've 
gathered but verify. On the subject of trust, what are some ways that poor communication can disrupt a team trying to manage an application?


--------------------------------------------------------------------------------------------------------------------------------------------------------------------

* Don't unplug this server
--------------------------
It's understandable to fear what you don't know. Fear of the unknown is a self-preservation instinct that protects us from harm. Without it, we just happily eat any mushroom we'd find in 
the woods, blindly run into dark rooms and engage in all sorts of incredibly risky behavior. 

We see messages like this all the time in real life usually with a story behind them. Don't open this door, don't turn this off and check with so-and-so before doing anything. 

There may be a busy hallway, a piece of electrically sensitive machinery, or a piece of equipment that requires special training to use. 

These messages can also be left as a form of control and a fear of delegation. 

This can be done with the best of intentions and sometimes for very good reasons, such as liability, sensitive information, and the need for a specific skillset. 

However, without documentation, process and a backup, a do not touch message can easily become a single point of failure. 

The message, Don't unplug the server, has a lot of hidden meanings within it. 

On the surface, it clearly states that nobody should stop the system. 

However, it doesn't say why the server shouldn't be unplugged. 

It doesn't state the purpose, the responsible party, or what will happen if the server is turned off. 

The path of least resistance is to blindly obey the message. Somebody else is responsible for this decision. I don't know who it is, but it's not me. 

These messages can exist in code as well. I've been guilty of this myself. I personally wrote, "Don't change this sorting algorithm unless you know exactly what you're doing." into a code 
base. 

That sort and code were well documented, but that doesn't change that this sort of message is disempowering and removes agency from others by making assumptions about the other 
maintainers' ability to adjust the code. 

In hindsight, that message was arrogant and caused more harm than good. This isn't to say that there shouldn't be messages warning of consequences. 

Systems can and do exist in various states of fragility and severity and disruptions can have massive consequences. 

If you ever need to leave a message of caution, give context and steps. Empower those who read it to know how to gracefully manage it. 

What will happen if things go badly, and additional resources. Often, a Legacy system won't have any type of messaging whatsoever. Does that mean there aren't consequences? Of course not. 

It just means that the consequences aren't labeled. I've definitely inherited systems that were forgotten experiments that didn't talk to anything and did nothing, but were hosted in 
production with uptime monitoring and emergency response expectations. 

The lack of messaging and the context of it being on in production meant people assumed it needed to be there. Do your best to infer the potential consequences if something were to happen 
to a system based on the traffic, contents and so forth. Document what you know with caveats of what are facts and what are assumptions. 

This will help build a strategy and foundation for managing the system moving forward. It's natural to obey a message of caution and it's healthy to question why. 

Knowledge is empowerment and you and others should have the ability to effectively manage a system. Legacy software is by its own definition, older. Is an older system a bad thing?

------------------------------------------------------------------------------------------------------------------------------------------------------------------

* Newer isn't necessarily better
--------------------------------
Newer isn't necessarily better. The fact that a system is older does not invalidate its use. Consider the perspective of respecting the history of an application. 

The application may be built differently than what you are accustomed to. It may not be using an exciting programming language or utilizing best practices. 

It may be objectively ugly and weird but does the application work? Can you look past a cloud of contempt for the slightest bit of rust and recognize the value that it brings? 

In Greek mythology, a king named Sisyphus was punished by the God of the dead for cheating death twice. His punishment was to roll a giant stone up a hill only for it to roll down. 
before it reached the top. 

Sisyphus was doomed to repeat this action for all eternity. The pursuit of the latest and greatest, the bleeding edge can be Sisyphean, an incomplete action doomed to be repeated forever. 

Arbitrarily. Installing the latest version of a package or implementing a given protocol without having a tangible impact can be a waste of time. 

Engineers can and should keep software current. But what is practical? What is current? It can be tempting to say current no matter what the cost, but there are costs and risks associated 
with every change. 

New versions of a system come with a potential for new bugs, unexpected behaviors and unintended consequences. 

Each time the system is changed, so does the risk profile. 

Security updates should be applied regularly even if the perception is that there is little or no risk to the organization. 

Estimations are exactly that and do leave room for error. There are some circumstances when there genuinely is no risk. 

I cannot decide for you when that time is. As it's dependent on the application, its audience, how people access it, the severity of the issue, the difficult and practicality of 
addressing it and the risk of being wrong. 

This is not to make the case for stagnation. Upgrades and updates can bring new life to an application. 

Over time, components can be upgraded or swapped out for more modern alternatives. 

An inconsistent code base can be refactored to adhere to current coding standards. 

Just because something is old doesn't mean it's effective. 

A critical eye is absolutely required, but with that balance of respect for what it can and does do. 

A system may be older, but if you can understand how it works and maintain it then a legacy system can be incredibly dependable. 

Consider the pure fact that it still exists all these years later. It survived for reasons. Of course, those reasons could be neglect, lack of actionable information and the fact that 
nobody remembered it existed. 

Refactors or rebuilds of a system can be incredibly time consuming and costly. 

There's absolutely a case to be made in favor of redoing a system, but it comes with inherent risk. 

An application built and maintained over years, likely has many business cases, edge cases, bug fixes and all sorts of refinements that have addressed a variety of concerns. 

When replacing a system like that, all of those scenarios and fixes need to be accounted for. 

How can you be certain that the replacement will surpass the original? A very possible outcome is an older system that people don't like working with, and a buggy and incomplete newer 
system that people prefer to work in but struggles to meet the functional requirements of the original. 

Like the creation of competing standards, if you take exception with one, instead of collaborating to address your concerns you create a new one. 

Now you have two standards. 

Is this progress? In summary, just because something is new doesn't mean it's superior. Carefully consider the cost of upgrading or replacing before making any changes. 

If it's not worth the cost in either direction there isn't an audience and there isn't a case to keep a system. It should be sunsetted and turned off. 

How can you safely determine if this is the time for a system to be shut down?

------------------------------------------------------------------------------------------------------------------------------------------------------------------

* Knowing when to retire a system
---------------------------------
Like life, software applications have a finite amount of time that they exist. The overall systems may persist, but individual components will come and go. 

How can you determine if a particular system has reached the end of its effectiveness? Sometimes the answer is incredibly obvious. 

If the application is serving no purpose, and isn't used by anything, then it's safe to turn off. 

It sounds pretty straightforward, but it's rare to find a system that is clearly ready to be turned off without serious research. 

Earlier, I discussed learning about the decision makers in determining who the audience of a system is either in people or in other services. 

If that audience is nobody, well, there's your answer, but what if the audience size is small or unknown? Instrumentation, logging and reporting can be used to determine the size of the 
current audience. 

Are people logging into the system? Is the API responding to requests, and if so, how many and what type, and who is requesting it? Is the system even actively running and accessible? 

How can you tell? Do an investigation. Based on those findings, have a conversation with the decision makers, even if that person is yourself. 

If the audience is small, how small is too small? 

What's the cost of maintaining the system compared to shutting it down? 

What functionality will be lost? 

Is that functionality or feature available somewhere else, either in your system or in someone else's? 

A third party perhaps? 

Is this something that can be delegated more effectively? 

If the system is malfunctioning, the repair is often easier than replacement. There's a difference between not wanting to have to deal with the system, and not needing that system. 

A system that is obtuse, difficult to work with, lacking documentation and error-prone will frustrate anyone. Developer experience is an incredibly important factor in maintainability. 

If the application is difficult to work with, then people will be reluctant to improve it. It's hard to prioritize frustrating work that doesn't offer direct value to an end user. 

Documentation, coding standards, test coverage and institutional knowledge, all add themselves to the developer experience. 

Bad code smells can be indicative of a greater problem with the system, and should be addressed instead of being allowed to fester. 

With that said, a negative developer experience is not a reason to retire a functioning system. 

Negative DX may impact how functional the system is, but shouldn't be the sole justification for ending support. 

Bad code smells should be addressed with a directed approach with a specific outcome. 

Improvements can take place over time, which may make scheduling more palatable when balancing delivering tangible improvements. 

If it truly is time to turn off a system, you can't just shut it off or delete it. 

Exercise good change management, and set expectations with stakeholders and decision-makers. Make a schedule and a strategy for how you're going to turn things off. 

Does it make sense to archive or migrate any content from this system to another? If there's a replacement, how can people or services be redirected to the alternative? 

What's the transition plan and timeline, and how will it be managed? If you're going to archive content from a system, include an expiration date, and get it on a known schedule. 

Otherwise, the system will be shut off, but will still be consuming resources, such as storage costs or database maintenance. 

When a system has been eliminated, have the plan to handle every last remaining part. If there's still an audience, and a purpose for the application, then it's time to shift focus toward 
the future. 

How can engineers rebuild an understanding of how a system works while improving it?

------------------------------------------------------------------------------------------------------------------------------------------------------------------

* Taking over an old system from the inside
-------------------------------------------
I enjoy narrative-driven experiences that reveal themselves incrementally, providing a series of discoveries that drive the story and lead to more questions, which then lead to even more. 

There is a joy in building knowledge, unraveling the puzzle to have solved the riddle or the challenge in order to achieve something. 

It can be immensely satisfying to conquer a problem and legacy software can absolutely be a riddle. 

Throughout my career as an engineer I've had numerous opportunities to explore and learn an existing system from the inside and outside. 

There's no one correct way, but there are some similarities in the approach that you can adapt to your needs. 

I'll give an example that describes both the challenges of maintaining legacy software and some strategies to handle it. 

Previously I mentioned an affiliate system. This was a project that I inherited when I joined a new company. 

It consisted of an event-driven tracking system, a data processing system, and a front end. 

It had been written as a replacement for another system by engineers who were no longer with the company, and nobody within the organization understood how it worked. 

Over time, it had struggled to keep up with traffic demands and was unable to keep current, falling further and further behind. 

The affiliate system needed both an emergency triage and a long-term understanding of how it functioned in order to bring it to a maintainable state. 

Before I continue, I'd like to stress that this system, despite the improvements necessary to make it sustainable, did provide a foundation that was used for at least five years 
afterward and may still be in use today. 

Giving respect where respect is due, the overall principles used were effective. 

It just took a bit of understanding and cleanup before it reached a more maintainable state. 

This is an example of an application that was brought back from the brink. 

With that said, I had my new responsibility and therefore got to work. The log messages were vague, sporadic, and confusing. The code was poorly documented and the system was incredibly 
abstracted and there was no manual. 

The triage pass involved determining what was taking the longest. 

With the assistance of other engineers we got the system running locally and through breakpoints and just reading code, set about adding instrumentation and logging messages. 

	. As we worked, we would add comments and method documentation about what they did and how to use them. 

	. We leveraged the IDE to refactor some variable names to have semantic meaning, so no more for i and foo statements. 

	. A manual was started with the bare minimum of how to run the system and how to test it. 

	. We sought out dead code, meaning code that was written but was never executed. 

	. Some of the labelings within the system caused confusion as well. 

Ostensibly, this was an extract, transform, and load data process. 

Yet the practical implementation was clearly a different model. A fair amount of time was spent trying to reconcile the labels throughout with what was actually happening. 

A cautionary tale about the importance of naming to be sure. Eventually, optimizations were found and applied which addressed the immediate problem. 

With the backlog of data processing addressed, the focus turned to the future. 

	. Organizational coding standards were applied. 

	. Automated testing was improved and expanded upon and the overall system was simplified. 

	. Throughout, the documentation continued to grow and other engineers contributed to the code base as well. 

There was absolutely a temptation to give up but the cost of failure was too great. 

It would've cost the company a terrifying amount of money and multiple jobs, including my own. 

This is not a tale of an individual accomplishment. 

While portions were handled by individuals, the responsibility and growth were shared through a supportive and collaborative team accomplishment. 

What are the takeaways from this experience? 

	. Start or expand on documentation the moment you begin. 

	. Triage immediate problems to get the system into a stable state. 

	. When the most critical issues are solved continue to document from the inside and out. 

	. Remove unreachable code. 

	. Perform small refactors to improve readability and clean up code formatting. 

	. Expand testing and the audience maintainers. 

	. Document the workflows from end-to-end in order to understand how it works, then find opportunities for optimization. 

As you're documenting features, what's a practical strategy to determine what's worth keeping?

------------------------------------------------------------------------------------------------------------------------------------------------------------

* The keep, kill, shrink exercise
---------------------------------
The act of documenting features in an existing system may seem pedantic, but it's an essential part of understanding what a system does. 

It naturally leads the conversation too. Should the system be doing what it's doing? The act of reducing, and more importantly, focusing the scope of an application increases its overall
maintainability. 

Why does it matter? 

Just because the system does a thing doesn't mean it should. Over time, it can be convenient to add extra functionality to an application that wasn't within the original scope. 

Tight deadlines, inexperienced engineers, confused expectations, and many other factors can combine to create some really awkward juxtapositions. 

While it should be avoided, it happens. 

Sometimes, a system is reinvented out of ignorance or out of necessity with no current alternatives. 

Over time, it may become clear that there's a much better solution available. 

No hard feelings are necessary. 

Just figure out the better, more sustainable solution and move on. 

There's an exercise that I learned about when I was working with a consulting agency, keep, kill, shrink. 

Practically, this can be done with a single spreadsheet. 

For each feature, describe its function, determine its importance, its complexity, and the risk of either implementation or deprecation. 

I prefer simple systems for measuring such things, such as high, medium, or low. 

It's arbitrary, but it provides a foundation for discussion. 

For example, a feature that is not important, highly complex, and very risky could be a candidate for removal. 

A very important feature that is complex should stay, but could be a candidate for simplification. 

Review this list with stakeholders and decision makers in order to determine its importance and also find out if they even knew it would do a particular task. 

These conversations can be deeply illuminating to all parties, especially given organizational turnover and a lack of institutional knowledge. 

Based on the outcome of these conversations and the rubric, the next step is to determine the fate of each feature. 

	. What would you like to keep? 

	. What should be removed? 

	. And what should be simplified? 

Of course, you should validate this understanding so something critical doesn't get removed accidentally. 

The results of this exercise can be prioritized in the context of the overall product. Implementation of any one of the action items, either kill or shrink, will take effort and potential 
change management and won't directly add benefit to end users. 

With that said, it does improve the maintainability and support burden that a system represents, which will in turn, lead to more opportunities for enhancements when complete. 

Find a balance that works best for you. 

As you're continuing to work with a system and you're finding opportunities to optimize and simplify, how can you be confident that the changes you're making won't have a negative 
impact?

-------------------------------------------------------------------------------------------------------------------------------------------------------------------

* Building confidence with automated testing
--------------------------------------------
A very common and reasonable impulse when faced with an unknown system is to make no changes whatsoever. 

That's almost never practical, given the probable need to improve, expand, or even remove components. 

In order to increase confidence in changes you're making to the system or its replacement, you're going to need test cases. 

A test case defines the steps for testing a particular system with an expected result. For example, I could have a test case for the acceptance criteria for a cat. 

I expect that a cat will meow. The result of a test case is a pass or fail, true or false. It either worked or it didn't. If it sort of works then the test case isn't specific enough. 

The result of what is being tested doesn't have to be a bullion. You could be checking for a specific value, a range of values, or maybe something that is greater than or equal. 

Regardless of whether the correctness is determined the end result is the same, it passed or failed. With a given test case, you need to apply it to the software in question. 

At a high level, there are two ways to test software, manually and automatically. When manually testing software, a human is given a set of instructions or test cases and goes through 
those steps to validate correctness. 

In contrast, test automation is the practice of using software driven tools to automate the execution of test cases to validate the correctness of a program. 

Test automation is separate software from the application that is being written and tested. 

Manual testing works, but has a number of drawbacks. The act of manual testing is time consuming, repetitive, error prone, and can be pretty boring. 

Humans can do comparisons quickly, but consider a process with dozens of steps and searching logs or generated output for a specific value. Manual testing is an essential part of software 
development. 

When writing test cases, they're often tested first manually. 

Test automation takes those test cases and enshrines them into code, typically in a different place than the actual program being tested. 

It can be in the same project sometimes organized in a different folder, and sometimes within the same folder, but with different file names. 

Regardless, automated test cases can be executed much more rapidly with higher confidence in consistency compared to manual testing. 

There are many different types of test cases. 

The most basic form is a unit test. 

Unit testing tests a completely isolated unit of code via its application program interface. Unit tests are performed in memory, meaning no permanent changes take place. 
This makes them safe to run over and over again. Additionally, unit tests execute very quickly, which makes them invaluable to development. 

If the project already has test cases, especially automated tests, that's fantastic. 

As you review them, you'll practically learn how the software is intended to work, hopefully along with edge cases and exceptional situations. 

If you don't have any test cases, well, you've got to start somewhere. 

Pick a component that could be easily isolated and write one test that compares an actual result with an expected result. 

One is better than none and writing the first is the hardest. If the project doesn't have an automated build process this would be an excellent opportunity to add one. 

At a minimum, add a test step to validate any changes that have been made, even if it's only the one test, it's still worth having. 

If there's any deviation from the expected result, test automation from a build process will highlight the problem before the changes are released. 

Building a suite of test cases is invaluable if you are looking to write a drop-in replacement for a system, especially for test cases that cover APIs. 

If you can guarantee that the API signature matches between the systems and produces the same results then the replacement will be completely transparent and should minimize any 
potential disruption. 

When you have test cases this empowers you to make changes to how the system works. 

Consider an optimization when you are taking an unnecessarily complicated process and simplifying it. 

The input and output should remain the same. It may be the case that you are the world's greatest programmer and your changes will perfectly optimize the system with no side effects. 

It's more likely that there may be an unintended side effect. Test your changes, preferably automatically to validate the correctness and reduce the possibility of defects. 

It's rarely necessary to test every single line of code in your system. 

Find a balance that will provide reasonable confidence that the mission critical paths of the application in user flow work as expected. 

Some tests are better than none and perfection is the enemy of good. Test efficiency is key in providing feedback, especially when working iteratively. 

If it takes a long time for tests to execute, engineers will procrastinate and avoid running tests to save time. Sometimes an older system is absolutely worth salvaging and maintaining 
rather than replacing outright. How can you go about improving an older system?

-------------------------------------------------------------------------------------------------------------------------------------------------------------

* Refactoring code in place
---------------------------
A code refactoring is when a software engineer restructures existing code without affecting its external behavior. 

There are many advantages to a refactor, including making the code easier to read and maintain, reducing the complexity, and in many cases, improving performance. 

Why would a refactor be necessary? Consider the act of building, well, anything. 

You start with a foundation and framework and build outwards. As you keep adding things, you start to focus on smaller and smaller components, filling in the fine details. 

As you progress, you're also learning and growing, and your approach to an earlier challenge may change with your experience. 

Additionally, coding practices, standards, and tooling change over time. 

You may look at a piece of code written five years ago that is still perfectly functional, but find that the style doesn't conform to today's standards. 

It doesn't mean that it's wrong, but it does mean that whenever you work in that system, do you conform to what was there or do you bring it up to the current standards? 

It can be considered good code hygiene to refactor and clean up code as you work continuously. 

A series of micro refactorings focused on fixing a particular problem can be incredibly effective. 

For example, renaming variables or methods to better reflect what they do should have no functional impact, but will improve readability. 

A larger piece of code could be broken into more logical pieces, improving maintenance and testability. 

In contrast, a monolithic refactor can be incredibly risky. Before considering a radical change, think critically about what the intended impact is. 

Then estimate the amount of time, the potential disruption, and risk the change represents. 

Is the work required to, say, rename every file to match a newer naming convention going to provide a tangible benefit? Or does it just arbitrarily check a box? Stylistic changes can also
represent a monolithic change. 

For example, standardizing spacing from tabs to spaces or using single quotes for string delimitation instead of double quotes could potentially impact every single file in a project. 

Ah, but I'm using an automated tool to make those stylistic changes, you may say to yourself, that's very low risk. It'll standardize the look and feel of the code. 

On the other hand, consider the impact this will have on the history of the code. If you look in a source control system at the history of a file, it will practically look like one 
engineer changed every one of those lines, because they did. 

If you're trying to review the history of a piece of code, you will have to go back to before that refactor to see how it was originally written. 

Regardless of the scope of a refactor, automated test cases should be set up before starting to change the code to ensure that the system still works as expected. 

Test automation provides confidence and stability to an engineer and empowers them to make incremental changes. 

Unit testing can empower an iterative cycle of refactoring. Make a small change, test it to ensure it's correct, then make another small transformation. 

A focused test suite that gives quick feedback is essential for this refactoring strategy. 

If you are maintaining a legacy system you likely didn't create it. 

Even if you did create it, the way it was originally designed may be different from your approach today. 

Systems written differently than how you would've approached them are just as valid. Different doesn't mean good or bad, it just is. 

You should tolerate stylistic and non-functional differences in the favor of stability. Change for the sake of change can just be a form of chaos. 

You are likely expected to deliver new functionality and move beyond the current challenges that you have in front of you. 

An unnecessary refactor can be costly, time-consuming, and frankly, a waste of time and effort that could be better spent doing something more valuable and innovative. 

Do make a point of looking for opportunities to improve pain points and maintainability. Don't just change a working system for the sake of changing it. 

If refactoring isn't worth the time or risk, it may be more logical to replace a system outright. It happens. Replicating a system is way easier than innovating and building something new 
for the first time. The same rules for refactoring apply to a replacement. Is the risk, time, effort, and so forth worth it? If it is, fantastic. 

How do you make a graceful switch between the old system and its replacement?

------------------------------------------------------------------------------------------------------------------------------------------------------------------

* Gracefully cutting over from old to new
-----------------------------------------
When re-implementing a system, you're likely going to be making different architectural decisions. 

If you're going to perfectly replicate the old system, why bother remaking it? There are likely very fundamental differences in how the new system is architected. 

This could be a different system for storage, caching, data structure, even programming language. 

Ultimately, how the new system is built doesn't matter as much as whether it fulfills the current intended purpose of the original system. 

What does matter is how the system is interacted with. A website front end is very visible to users who will immediately notice if things start looking different. 

If the intent is to stay true to the original look and feel, then care needs to be taken to ensure consistency between the systems. 

An application programming interface, also known as an API, is less nuanced in comparison. Any clients connecting to the API expect it to work in a particular way. 

If a route is missing or responds differently, chaos ensues in the form of impaired functionality at best and data loss at worst. 

Often, it's not practical or possible to refactor the clients at the same time to compensate for a different architecture. 

When developing a replacement system, there are likely very good reasons to get the updated version shipped as soon as possible. 

However, the act of creating an application takes time, even if you have a prototype that you're following. Reaching functional parody can potentially take a very long time. 

Acceptance testing and validation will take even longer and a cut over may require migration and downtime. 

Is a monolithic cut over the only approach? The strangler fig pattern was coined by Martin Fowler, a software engineer. 

A strangler fig is a tropical plant that seeds in the upper branches of a tree then gradually grows down until it roots in the soil. 

Over time, the strangler fig feeds on and takes over the host, usually resulting in the death of the original tree. 

With that context, the strangler fig pattern is an industry standard approach for backend and front end, in place system takeovers for breaking up monoliths and incrementally removing 
control. 

This alternate route gradually creates a new system around the edges of the old and grows with time. 

Instead of a seed, a proxy is placed over the old system. 

When a new service or implementation is ready, the proxy defers the relevant traffic to the new service. Over time, proxying and swapping continue until all the relevant parts of the 
legacy system have been migrated. 

When there are no remaining parts, the host tree has been consumed and the legacy application can be decommissioned. 

This approach allows for steady and frequent releases for an incremental approach, which in turn reduces risks. Making the call to turn off a legacy application can be difficult without 
complete knowledge of what it was doing. 

Paralysis and indecision, like what I outlined and don't unplug this server, can add years of life to a potentially deprecated system. There's a technique called the scream test where you 
turn a system off and wait for the screams and panic. 

If nobody screams, it validates that the system was safe to turn off. 

If someone did scream, then you can absolutely turn it back on and note who that stakeholder was. The scream test isn't a guarantee, but it can be effective. 

If you attempted, there should always be a rollback strategy and backups. Here's a practical example of this type of situation. 

The current generation of a website consisted of microservices, a renderer, and a content management system. 

The previous version of that same website was a monolith, with everything integrated into a single gigantic application. 

Bit by bit and piece by piece, functionality was transferred into the new. 

In fact, a whole initiative had gone into deprecating the old infrastructure. 

Engineers had decoupled the front end into a distinct rendering system and there was a standalone content management system. Yet several years later, the system remained. 

What was it doing? The last time the scream test was attempted, a public facing file download system failed to work. Priority wasn't given to the investigation, so the old system 
continued to operate. 

A couple of years later, I inherited this application along with part of its successor. 

I interviewed a number of people for institutional history, red server logs, and some of the original source code. 

I eventually determined that the file downloads were relying on the legacy system to look up a record, determine the file type, and set the file delivery to the mime type. 

I worked with a relevant team to replace the lookup with some proxy rules that would set the file type based on the file extension. The fix went out and was validated, then the scream 
test was repeated. 

We listened and there was silence. This service was decommissioned gracefully. What's the moral of the story? 

	. The final finishing touches of any project can take the longest and can easily be disrupted. 

	. Persistence and iteration are key. 

	. At this point, you should either have a functioning and streamlined older application, a newer replacement application, or hybrid of the two and a gradual cut over strategy. 

Should you always be personally responsible for the system? How do you want to be remembered?

------------------------------------------------------------------------------------------------------------------------------------------------------------------












































































































































































































